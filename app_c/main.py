#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
researchmap JSON â†’ Excel å¤‰æ› Web ã‚¢ãƒ—ãƒª

Usage:
  uvicorn app_c.main:app --reload --port 8001

ã‚µãƒ–ãƒ‘ã‚¹é…ä¸‹ã§ã®é‹ç”¨:
  NIHU_RM_ROOT_PATH=/nihu-rm-c uvicorn app_c.main:app --port 8001

ã‚¢ã‚¯ã‚»ã‚¹:
  http://localhost:8001/
"""

import os
import re
import shutil
import uuid
from pathlib import Path

from dotenv import load_dotenv
import httpx
from fastapi import FastAPI, Form, HTTPException, BackgroundTasks

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã® .env ã‚’èª­ã¿è¾¼ã¿
PROJECT_ROOT = Path(__file__).parent.parent
load_dotenv(PROJECT_ROOT / ".env")
from fastapi.responses import FileResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from starlette.requests import Request

# å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªèº«ï¼‰
SCRIPT_DIR = Path(__file__).parent

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦å®Ÿè¡Œã—ãŸå ´åˆã«å‚™ãˆã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ sys.path ã«è¿½åŠ 
import sys
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

# app_a ã®ãƒ‘ã‚¹ã‚’è¿½åŠ ï¼ˆdownload_data.py ã®å‚ç…§ç”¨ï¼‰
APP_A_DIR = Path(__file__).parent.parent / "app_a"
if str(APP_A_DIR) not in sys.path:
    sys.path.insert(0, str(APP_A_DIR))

from common import (
    get_researchmap_data,
    get_profile_fields,
)
from download_data import fetch_researcher_data

# root_path ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆnginx ã§ã‚µãƒ–ãƒ‘ã‚¹é…ä¸‹ã«é…ç½®ã™ã‚‹å ´åˆã«è¨­å®šï¼‰
ROOT_PATH = os.environ.get("NIHU_RM_ROOT_PATH", "")

app = FastAPI(
    title="researchmap â†’ Excel å¤‰æ›",
    description="researchmap ãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ§‹IRæ§˜å¼ã® Excel ã«å¤‰æ›ã™ã‚‹ API\n\n"
                f"ğŸ”— [å¤‰æ›ç”»é¢ã‚’é–‹ã]({ROOT_PATH}/)" if ROOT_PATH else
                "researchmap ãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ§‹IRæ§˜å¼ã® Excel ã«å¤‰æ›ã™ã‚‹ API",
    root_path=ROOT_PATH,
    docs_url="/docs",
    openapi_url="/openapi.json",
)

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨é™çš„ãƒ•ã‚¡ã‚¤ãƒ«
templates = Jinja2Templates(directory=Path(__file__).parent / "templates")
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "static"), name="static")

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
WORK_DIR = Path(__file__).parent / "work"
WORK_DIR.mkdir(exist_ok=True)

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = Path(__file__).parent.parent / "data"

# researchmap API è¨­å®š
RESEARCHMAP_API_BASE = "https://api.researchmap.jp"


def _extract_permalinks_from_url_csv(csv_path: Path) -> set[str]:
    """CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ researchmap URL ã‚’æ¤œå‡ºã— permalink ã‚’æŠ½å‡ºã™ã‚‹"""
    permalinks = set()
    if not csv_path.exists():
        return permalinks

    with csv_path.open("r", encoding="utf-8-sig") as f:
        for line in f:
            # https://researchmap.jp/{permalink} ã‹ã‚‰ permalink ã‚’æŠ½å‡º
            match = re.search(r"https://researchmap\.jp/([^/,\s\"]+)", line)
            if match:
                permalink = match.group(1)
                # avatar ãªã©ã‚’é™¤å¤–
                if not permalink.endswith((".jpg", ".png", ".gif")):
                    permalinks.add(permalink)
    return permalinks


def _extract_permalinks_from_header_tsv(tsv_path: Path, column_name: str = "rm_id") -> set[str]:
    """ãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ã TSVï¼ˆã‚¿ãƒ–åŒºåˆ‡ã‚Šï¼‰ã‹ã‚‰æŒ‡å®šã‚«ãƒ©ãƒ ã®å€¤ã‚’æŠ½å‡ºã™ã‚‹"""
    permalinks = set()
    if not tsv_path.exists():
        print(f"Warning: TSV file not found: {tsv_path}", file=sys.stderr)
        return permalinks

    with tsv_path.open("r", encoding="utf-8-sig") as f:
        lines = f.readlines()

    if not lines:
        print(f"Warning: TSV file is empty: {tsv_path}", file=sys.stderr)
        return permalinks

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è§£æ
    header_line = lines[0].strip()
    if "\t" not in header_line:
        print(f"Warning: TSV file is not tab-delimited: {tsv_path}", file=sys.stderr)
        print(f"  Header line: {repr(header_line)}", file=sys.stderr)
        return permalinks

    headers = [h.strip() for h in header_line.split("\t")]
    if column_name not in headers:
        print(f"Warning: Column '{column_name}' not found in TSV: {tsv_path}", file=sys.stderr)
        print(f"  Available columns: {headers}", file=sys.stderr)
        return permalinks
    col_idx = headers.index(column_name)

    # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†
    for line_num, line in enumerate(lines[1:], start=2):
        fields = line.strip().split("\t")
        if col_idx < len(fields):
            value = fields[col_idx].strip()
            if value:
                permalinks.add(value)

    return permalinks


def load_allowed_ids() -> set[str]:
    """CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨±å¯ã•ã‚ŒãŸ permalink ã‚’èª­ã¿è¾¼ã‚€

    èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã€data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰:
      - TOOL_C_MAIN_CSV: ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆï¼ˆURLå½¢å¼ï¼‰
      - TOOL_C_ADD_CSV: è¿½åŠ ãƒªã‚¹ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
      - TOOL_C_ADD_CSV_COLUMN: è¿½åŠ ãƒªã‚¹ãƒˆã®ã‚«ãƒ©ãƒ åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: rm_idï¼‰

    CSVä»•æ§˜: .env-sample ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§
    """
    allowed = set()

    # ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆï¼ˆURLå½¢å¼ï¼‰
    main_csv_name = os.environ.get("TOOL_C_MAIN_CSV", "").strip()
    if main_csv_name:
        main_csv = DATA_DIR / main_csv_name
        main_ids = _extract_permalinks_from_url_csv(main_csv)
        allowed.update(main_ids)
        print(f"Loaded {len(main_ids)} IDs from main CSV: {main_csv_name}", file=sys.stderr)
    else:
        print("Warning: TOOL_C_MAIN_CSV not set", file=sys.stderr)

    # è¿½åŠ ãƒªã‚¹ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ãï¼‰
    add_csv_name = os.environ.get("TOOL_C_ADD_CSV", "").strip()
    if add_csv_name:
        add_csv = DATA_DIR / add_csv_name
        column_name = os.environ.get("TOOL_C_ADD_CSV_COLUMN", "rm_id").strip()
        add_ids = _extract_permalinks_from_header_tsv(add_csv, column_name)
        allowed.update(add_ids)
        print(f"Loaded {len(add_ids)} IDs from add CSV: {add_csv_name}", file=sys.stderr)

    print(f"Total allowed IDs: {len(allowed)}", file=sys.stderr)
    return allowed


# è¨±å¯ã•ã‚ŒãŸ ID ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
ALLOWED_IDS: set[str] = load_allowed_ids()


def validate_researcher_id(researcher_id: str) -> str:
    """ç ”ç©¶è€… ID ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    researcher_id = researcher_id.strip()

    if not researcher_id:
        raise HTTPException(status_code=400, detail="ç ”ç©¶è€… ID ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # è¨±å¯ãƒªã‚¹ãƒˆã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if researcher_id not in ALLOWED_IDS:
        raise HTTPException(
            status_code=400,
            detail=f"'{researcher_id}' ã¯è¨±å¯ã•ã‚ŒãŸç ”ç©¶è€… ID ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚CSV ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ ID ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚"
        )

    return researcher_id


async def download_researchmap_json(researcher_id: str) -> dict:
    """researchmap API ã‹ã‚‰ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆdownload_data.py ã‚’å†åˆ©ç”¨ï¼‰"""
    async with httpx.AsyncClient(timeout=60.0) as client:
        rm_data = await fetch_researcher_data(client, researcher_id)

        if not rm_data or "profile" not in rm_data:
            raise HTTPException(
                status_code=404,
                detail=f"ç ”ç©¶è€… ID '{researcher_id}' ãŒ researchmap ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        # researchmap_data å½¢å¼ã§ãƒ©ãƒƒãƒ—ï¼ˆæ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
        return {"researchmap_data": rm_data}


def convert_json_to_csvs(json_data: dict, output_dir: Path, researcher_id: str) -> list[Path]:
    """JSON ã‚’å„ç¨® CSV ã«å¤‰æ›ï¼ˆsubprocess ã§å„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œï¼‰"""
    import json
    import subprocess

    # JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
    json_path = output_dir / f"{researcher_id}.json"
    with json_path.open("w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    csv_dir = output_dir / "csv"
    csv_dir.mkdir(exist_ok=True)

    # å„å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ãã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    converters = [
        ("researchmap_json_to_csv_papers.py", "è«–æ–‡"),
        ("researchmap_json_to_csv_buntan.py", "åˆ†æ‹…åŸ·ç­†"),
        ("researchmap_json_to_csv_tancho.py", "å˜è‘—"),
        ("researchmap_json_to_csv_kyocho_hencho.py", "å…±è‘—ç·¨è‘—"),
        ("researchmap_json_to_csv_kotohappyo.py", "å£é ­ç™ºè¡¨"),
        ("researchmap_json_to_csv_misc.py", "MISC"),
        ("researchmap_json_to_csv_sonota.py", "ãã®ä»–"),
    ]

    csv_files = []
    for script_name, category in converters:
        script_path = SCRIPT_DIR / script_name
        if not script_path.exists():
            print(f"Warning: Script not found: {script_path}")
            continue

        try:
            result = subprocess.run(
                [
                    sys.executable,
                    str(script_path),
                    "--input-file", str(json_path),
                    "--output-dir", str(csv_dir),
                ],
                capture_output=True,
                text=True,
                timeout=60,
                cwd=str(SCRIPT_DIR),  # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œ
            )

            if result.returncode != 0:
                print(f"Warning: {script_name} failed: {result.stderr}")
                continue

            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            out_path = csv_dir / f"{researcher_id}-{category}.csv"
            if out_path.exists():
                csv_files.append(out_path)
            else:
                print(f"Warning: Output not found: {out_path}")

        except subprocess.TimeoutExpired:
            print(f"Warning: {script_name} timed out")
        except Exception as e:
            print(f"Warning: {script_name} error: {e}")

    return csv_files


def convert_csvs_to_excel(csv_dir: Path, output_dir: Path, researcher_id: str, fiscal_year: int = None) -> Path:
    """CSV ã‚’ Excel ã«å¤‰æ›ï¼ˆcreate_researcher_excel é–¢æ•°ã‚’ç›´æ¥å‘¼ã³å‡ºã—ï¼‰

    Args:
        csv_dir: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        researcher_id: ç ”ç©¶è€…ID
        fiscal_year: å¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¾‹: 2023 = 2023å¹´4æœˆã€œ2024å¹´3æœˆï¼‰ã€Noneã§ãƒ•ã‚£ãƒ«ã‚¿ãªã—
    """
    import importlib.util

    xlsx_dir = output_dir / "xlsx"
    xlsx_dir.mkdir(exist_ok=True)

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
    csv_files = list(csv_dir.glob(f"{researcher_id}-*.csv"))
    if not csv_files:
        raise HTTPException(status_code=500, detail="CSV ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†
    category_files = {}
    for csv_file in csv_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º (ä¾‹: cm3-è«–æ–‡.csv -> è«–æ–‡)
        category = csv_file.stem.replace(f"{researcher_id}-", "")
        category_files[category] = csv_file

    # csv_to_excel ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‹•çš„ã«ãƒ­ãƒ¼ãƒ‰
    spec = importlib.util.spec_from_file_location("csv_to_excel", SCRIPT_DIR / "csv_to_excel.py")
    csv_to_excel = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(csv_to_excel)

    # Excel ä½œæˆ
    try:
        csv_to_excel.create_researcher_excel(researcher_id, category_files, xlsx_dir, fiscal_year=fiscal_year)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Excel å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ±ºå®š
    if fiscal_year:
        xlsx_path = xlsx_dir / f"{researcher_id}_{fiscal_year}å¹´åº¦.xlsx"
    else:
        xlsx_path = xlsx_dir / f"{researcher_id}.xlsx"

    if not xlsx_path.exists():
        raise HTTPException(status_code=500, detail=f"Excel ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    return xlsx_path


def cleanup_old_files():
    """å¤ã„ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    import time
    max_age_hours = int(os.environ.get("TOOL_C_WORK_MAX_AGE_HOURS", "24"))
    now = time.time()
    max_age_seconds = max_age_hours * 3600

    for item in WORK_DIR.iterdir():
        if item.is_dir():
            try:
                age = now - item.stat().st_mtime
                if age > max_age_seconds:
                    shutil.rmtree(item)
            except Exception:
                pass


@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api/allowed-ids")
async def get_allowed_ids():
    """è¨±å¯ã•ã‚ŒãŸ ID ä¸€è¦§ã‚’è¿”ã™"""
    return {"ids": sorted(ALLOWED_IDS), "count": len(ALLOWED_IDS)}


@app.post("/api/convert")
async def convert(
    researcher_id: str = Form(...),
    fiscal_year: str = Form(None),
    background_tasks: BackgroundTasks = None
):
    """ç ”ç©¶è€… ID ã‹ã‚‰ Excel ã‚’ç”Ÿæˆ

    Args:
        researcher_id: researchmap ã®ç ”ç©¶è€… ID
        fiscal_year: å¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¾‹: "2023" = 2023å¹´4æœˆã€œ2024å¹´3æœˆï¼‰ã€ç©ºæ–‡å­—ã¾ãŸã¯ None ã§ãƒ•ã‚£ãƒ«ã‚¿ãªã—
    """
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    researcher_id = validate_researcher_id(researcher_id)

    # å¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ã®ãƒ‘ãƒ¼ã‚¹
    fiscal_year_int = None
    if fiscal_year and fiscal_year.strip():
        try:
            fiscal_year_int = int(fiscal_year.strip())
        except ValueError:
            raise HTTPException(status_code=400, detail=f"å¹´åº¦ã®å½¢å¼ãŒä¸æ­£ã§ã™: {fiscal_year}")

    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    work_id = str(uuid.uuid4())[:8]
    work_path = WORK_DIR / work_id
    work_path.mkdir(parents=True, exist_ok=True)

    try:
        # 1. JSON ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        json_data = await download_researchmap_json(researcher_id)

        # 2. JSON â†’ CSV å¤‰æ›
        csv_dir = work_path / "csv"
        csv_files = convert_json_to_csvs(json_data, work_path, researcher_id)

        # 3. CSV â†’ Excel å¤‰æ›ï¼ˆå¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ï¼‰
        xlsx_path = convert_csvs_to_excel(csv_dir, work_path, researcher_id, fiscal_year=fiscal_year_int)

        if not xlsx_path.exists():
            raise HTTPException(status_code=500, detail="Excel ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
        rm_data = get_researchmap_data(json_data.get("researchmap_data", json_data))
        name, erad_id = get_profile_fields(rm_data.get("profile"))

        # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
        if background_tasks:
            background_tasks.add_task(cleanup_old_files)

        return {
            "success": True,
            "researcher_id": researcher_id,
            "name": name,
            "erad_id": erad_id,
            "fiscal_year": fiscal_year_int,
            "download_url": f"api/download/{work_id}/{xlsx_path.name}",
            "csv_count": len(csv_files),
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/download/{work_id}/{filename}")
async def download(work_id: str, filename: str):
    """ç”Ÿæˆã—ãŸ Excel ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    file_path = WORK_DIR / work_id / "xlsx" / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return FileResponse(
        path=file_path,
        filename=filename,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )


if __name__ == "__main__":
    import uvicorn
    # é–‹ç™ºæ™‚ã¯ç›´æ¥å®Ÿè¡Œå¯èƒ½
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        root_path=ROOT_PATH,
    )
